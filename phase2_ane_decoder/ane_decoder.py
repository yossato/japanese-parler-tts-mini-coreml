#!/usr/bin/env python3
"""
Phase 2: TTSãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ANEï¼ˆApple Neural Engineï¼‰ã§å‹•ä½œã•ã›ã‚‹
å›ºå®šé•·ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’Core MLã«å¤‰æ›ã—ã¦ANEã§å®Ÿè¡Œ
"""

import sys
import os
sys.path.append('/Users/yoshiaki/Projects/parler-tts')
sys.path.append('/Users/yoshiaki/Projects/parler_tts_experiment/phase1_fixed_length')

import torch
import torch.nn as nn
import coremltools as ct
import numpy as np
from fixed_length_decoder import FixedLengthTTSDecoder, SimplifiedTTSGenerator
import time
import json

class ANEOptimizedTTSDecoder(nn.Module):
    """ANEæœ€é©åŒ–ã•ã‚ŒãŸTTSãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, hidden_size=1024, num_codebooks=9, vocab_size=1088, max_length=500):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_codebooks = num_codebooks
        self.vocab_size = vocab_size
        self.max_length = max_length
        
        # ANEã«æœ€é©åŒ–ã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
        # 1. ã‚·ãƒ³ãƒ—ãƒ«ãªå…¨çµåˆå±¤ã‚’ä½¿ç”¨ï¼ˆANEãŒå¾—æ„ï¼‰
        self.encoder_projection = nn.Linear(hidden_size, hidden_size)
        self.activation = nn.GELU()  # ANEã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹æ´»æ€§åŒ–é–¢æ•°
        
        # 2. å„codebookã”ã¨ã«åˆ†é›¢ã•ã‚ŒãŸheads
        self.codebook_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.GELU(),
                nn.Linear(hidden_size // 2, max_length * vocab_size)
            ) for _ in range(num_codebooks)
        ])
        
        # 3. é•·ã•äºˆæ¸¬å™¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.length_predictor = nn.Sequential(
            nn.Linear(hidden_size, 64),
            nn.GELU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, encoder_hidden_states):
        """
        Args:
            encoder_hidden_states: [batch_size, seq_len, hidden_size]
        
        Returns:
            codebook_outputs: [batch_size, num_codebooks, max_length]
        """
        # Global pooling (ANEã«æœ€é©åŒ–)
        pooled = encoder_hidden_states.mean(dim=1)  # [batch_size, hidden_size]
        
        # Encoder projection
        projected = self.activation(self.encoder_projection(pooled))
        
        # é•·ã•äºˆæ¸¬
        predicted_length = self.length_predictor(projected)
        
        # å„codebookã®å‡ºåŠ›ç”Ÿæˆ
        codebook_outputs = []
        for head in self.codebook_heads:
            # [batch_size, max_length * vocab_size]
            head_output = head(projected)
            
            # [batch_size, max_length, vocab_size]
            reshaped = head_output.view(-1, self.max_length, self.vocab_size)
            
            # Argmax to get token IDs
            tokens = torch.argmax(reshaped, dim=-1)
            codebook_outputs.append(tokens)
        
        # [batch_size, num_codebooks, max_length]
        result = torch.stack(codebook_outputs, dim=1)
        
        return result, predicted_length

class TTSDecoderCoreMLConverter:
    """TTS Decoder Core MLå¤‰æ›å™¨"""
    
    def __init__(self):
        self.batch_size = 1
        self.seq_length = 50  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å‡ºåŠ›ã®æœ€å¤§é•·
        self.hidden_size = 1024
        self.max_length = 500
        self.num_codebooks = 9
        self.vocab_size = 1088
    
    def create_ane_optimized_model(self):
        """ANEæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        model = ANEOptimizedTTSDecoder(
            hidden_size=self.hidden_size,
            num_codebooks=self.num_codebooks,
            vocab_size=self.vocab_size,
            max_length=self.max_length
        )
        model.eval()
        return model
    
    def convert_to_coreml(self, pytorch_model):
        """PyTorchãƒ¢ãƒ‡ãƒ«ã‚’Core MLã«å¤‰æ›"""
        
        # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ä½œæˆ
        sample_input = torch.randn(
            self.batch_size, 
            self.seq_length, 
            self.hidden_size
        )
        
        print("Converting to Core ML...")
        print(f"Input shape: {sample_input.shape}")
        
        # ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å®Ÿè¡Œ
        with torch.no_grad():
            traced_model = torch.jit.trace(pytorch_model, sample_input)
            
            # å‡ºåŠ›ç¢ºèª
            sample_output = traced_model(sample_input)
            print(f"Output shapes: {[out.shape for out in sample_output]}")
        
        # Core MLå¤‰æ›
        coreml_model = ct.convert(
            traced_model,
            inputs=[
                ct.TensorType(
                    name="encoder_hidden_states",
                    shape=(self.batch_size, self.seq_length, self.hidden_size),
                    dtype=np.float32
                )
            ],
            outputs=[
                ct.TensorType(name="audio_tokens", dtype=np.int32),
                ct.TensorType(name="predicted_length", dtype=np.float32)
            ],
            compute_units=ct.ComputeUnit.ALL,  # ANEä½¿ç”¨
            minimum_deployment_target=ct.target.macOS13,  # macOS 13ä»¥ä¸Š
            convert_to="neuralnetwork"  # Neural Networkå½¢å¼ï¼ˆANEæœ€é©åŒ–ï¼‰
        )
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¨­å®š
        coreml_model.author = "Parler TTS ANE Experiment"
        coreml_model.short_description = "ANE-optimized TTS Decoder"
        coreml_model.version = "1.0"
        
        return coreml_model
    
    def optimize_for_ane(self, coreml_model):
        """ANEå‘ã‘æœ€é©åŒ–"""
        
        # é‡å­åŒ–è¨­å®šï¼ˆANEã§ã¯16bit floatãŒåŠ¹ç‡çš„ï¼‰
        try:
            # 8bité‡å­åŒ–ã‚’è©¦è¡Œ
            quantized_model = ct.models.neural_network.quantization_utils.quantize_weights(
                coreml_model, 
                nbits=8
            )
            print("8-bit quantization applied")
            return quantized_model
        except Exception as e:
            print(f"Quantization failed: {e}, using original model")
            return coreml_model
    
    def benchmark_coreml_model(self, coreml_model, num_runs=10):
        """Core MLãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        
        print(f"Benchmarking Core ML model ({num_runs} runs)...")
        
        # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›æº–å‚™
        sample_input = np.random.randn(
            self.batch_size, 
            self.seq_length, 
            self.hidden_size
        ).astype(np.float32)
        
        import coremltools.models as ct_models
        
        times = []
        
        for i in range(num_runs):
            start_time = time.time()
            
            # Core MLæ¨è«–å®Ÿè¡Œ
            input_dict = {"encoder_hidden_states": sample_input}
            result = coreml_model.predict(input_dict)
            
            end_time = time.time()
            times.append(end_time - start_time)
            
            if i == 0:
                print(f"Output keys: {result.keys()}")
                for key, value in result.items():
                    if hasattr(value, 'shape'):
                        print(f"{key} shape: {value.shape}")
        
        avg_time = sum(times) / len(times)
        print(f"Average inference time: {avg_time*1000:.2f} ms")
        print(f"Min time: {min(times)*1000:.2f} ms") 
        print(f"Max time: {max(times)*1000:.2f} ms")
        
        return {
            'avg_time_ms': avg_time * 1000,
            'times_ms': [t * 1000 for t in times],
            'min_time_ms': min(times) * 1000,
            'max_time_ms': max(times) * 1000
        }

class HybridTTSPipeline:
    """PyTorch + Core ML ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, coreml_decoder_path):
        # PyTorchã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆPhase 3ã§ç½®ãæ›ãˆäºˆå®šï¼‰
        self.pytorch_generator = SimplifiedTTSGenerator()
        
        # Core MLãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
        import coremltools as ct
        self.coreml_decoder = ct.models.MLModel(coreml_decoder_path)
        
        print("Hybrid pipeline initialized")
    
    def generate_with_ane_decoder(self, text, description="A female speaker delivers her words expressively."):
        """ANEãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ"""
        
        # Phase 1: PyTorchã§ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        text_inputs = self.pytorch_generator.tokenizer(
            description,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=50
        )
        
        with torch.no_grad():
            encoder_outputs = self.pytorch_generator.model.text_encoder(
                input_ids=text_inputs.input_ids,
                attention_mask=text_inputs.attention_mask
            )
        
        # Phase 2: Core ML (ANE) ã§ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        encoder_hidden_states = encoder_outputs.last_hidden_state.numpy()
        
        start_time = time.time()
        coreml_result = self.coreml_decoder.predict({
            "encoder_hidden_states": encoder_hidden_states
        })
        ane_time = time.time() - start_time
        
        return {
            'audio_tokens': coreml_result['audio_tokens'],
            'predicted_length': coreml_result['predicted_length'],
            'ane_inference_time': ane_time
        }

def run_phase2_experiment():
    """Phase 2å®Ÿé¨“ã®å®Ÿè¡Œ"""
    
    print("Phase 2: ANE TTS Decoder Experiment")
    print("=" * 50)
    
    # 1. ANEæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    converter = TTSDecoderCoreMLConverter()
    pytorch_model = converter.create_ane_optimized_model()
    
    # 2. Core MLå¤‰æ›
    print("\n1. Converting to Core ML...")
    coreml_model = converter.convert_to_coreml(pytorch_model)
    
    # 3. ANEæœ€é©åŒ–
    print("\n2. Optimizing for ANE...")
    optimized_model = converter.optimize_for_ane(coreml_model)
    
    # 4. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = "ane_tts_decoder.mlmodel"
    optimized_model.save(model_path)
    print(f"Model saved to: {model_path}")
    
    # 5. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    print("\n3. Benchmarking Core ML model...")
    benchmark_results = converter.benchmark_coreml_model(optimized_model, num_runs=10)
    
    # 6. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
    print("\n4. Testing hybrid pipeline...")
    try:
        pipeline = HybridTTSPipeline(model_path)
        
        test_text = "Hello world, this is a test."
        result = pipeline.generate_with_ane_decoder(test_text)
        
        hybrid_results = {
            'audio_tokens_shape': result['audio_tokens'].shape,
            'predicted_length': float(result['predicted_length'][0]),
            'ane_inference_time_ms': result['ane_inference_time'] * 1000
        }
        
        print(f"Hybrid generation successful!")
        print(f"Audio tokens shape: {hybrid_results['audio_tokens_shape']}")
        print(f"ANE inference time: {hybrid_results['ane_inference_time_ms']:.2f} ms")
        
    except Exception as e:
        print(f"Hybrid pipeline test failed: {e}")
        hybrid_results = {'error': str(e)}
    
    # 7. çµæœä¿å­˜
    results = {
        'phase': 2,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'coreml_benchmark': benchmark_results,
        'hybrid_pipeline': hybrid_results,
        'model_path': model_path
    }
    
    with open('phase2_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… Phase 2 completed! Results saved to phase2_results.json")
    
    return results

if __name__ == "__main__":
    try:
        results = run_phase2_experiment()
        
        print("\nğŸ“Š Summary:")
        if 'coreml_benchmark' in results:
            print(f"ANE Decoder Average Time: {results['coreml_benchmark']['avg_time_ms']:.2f} ms")
        
        if 'error' not in results.get('hybrid_pipeline', {}):
            print("âœ… Hybrid pipeline working successfully")
        else:
            print("âŒ Hybrid pipeline needs debugging")
        
        print("\nNext steps:")
        print("1. Compare ANE decoder speed with PyTorch version")
        print("2. Verify output quality")
        print("3. Proceed to Phase 3 for full ANE conversion")
        
    except Exception as e:
        print(f"âŒ Phase 2 experiment failed: {e}")
        import traceback
        traceback.print_exc()
