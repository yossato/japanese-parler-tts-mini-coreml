
# æœ€çµ‚çš„ã«å‹•ä½œã™ã‚‹DACçµ±åˆã‚³ãƒ¼ãƒ‰
def codes_to_audio(self, audio_codes):
    '''éŸ³å£°ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å®Ÿéš›ã®éŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆ (æœ€çµ‚å‹•ä½œç‰ˆ)'''
    
    print(f"ğŸµ Converting codes to audio...")
    print(f"   Input codes shape: {audio_codes.shape}")
    
    with torch.no_grad():
        start_time = time.time()
        
        try:
            # DACå†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            dac_internal = self.audio_encoder.model
            quantizer = dac_internal.quantizer
            
            # éŸ³å£°ã‚³ãƒ¼ãƒ‰ã®å½¢çŠ¶ã‚’èª¿æ•´
            if audio_codes.dim() == 2:
                audio_codes = audio_codes.unsqueeze(0)  # [9, 100] -> [1, 9, 100]
            
            # éŸ³å£°ã‚³ãƒ¼ãƒ‰ã‚’æ­£ã—ã„ç¯„å›²ã«ã‚¯ãƒ©ãƒ³ãƒ—
            audio_codes = torch.clamp(audio_codes, 0, self.dac_config.codebook_size - 1)
            print(f"   Clamped codes range: [{audio_codes.min()}, {audio_codes.max()}]")
            
            # å„quantizerãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ‰‹å‹•ã§å‡¦ç†
            latents = torch.zeros(audio_codes.shape[0], 1024, audio_codes.shape[2], 
                                device=audio_codes.device)
            
            for i, quantizer_layer in enumerate(quantizer.quantizers):
                if i < audio_codes.shape[1]:
                    layer_codes = audio_codes[:, i, :]  # [1, 100]
                    
                    # åŸ‹ã‚è¾¼ã¿å‡¦ç†
                    if hasattr(quantizer_layer, 'embed'):
                        embedded = quantizer_layer.embed(layer_codes)
                    else:
                        embedded = quantizer_layer.codebook(layer_codes)
                        embedded = embedded.transpose(1, 2)  # [1, time, dim] -> [1, dim, time]
                    
                    latents += embedded
            
            print(f"   Combined latents shape: {latents.shape}")
            
            # DAC decodeå®Ÿè¡Œ
            audio_waveform = dac_internal.decode(latents.detach())
            
            if isinstance(audio_waveform, torch.Tensor):
                audio_np = audio_waveform.detach().squeeze().cpu().numpy()
            else:
                audio_np = np.array(audio_waveform)
            
            end_time = time.time()
            
            print(f"   â±ï¸  Audio conversion time: {end_time - start_time:.3f}s")
            print(f"   ğŸ“Š Audio waveform shape: {audio_np.shape}")
            print(f"   ğŸšï¸  Audio range: [{audio_np.min():.3f}, {audio_np.max():.3f}]")
            
            return {
                'audio_waveform': audio_np,
                'conversion_time': end_time - start_time,
                'sample_rate': self.dac_config.sampling_rate,
                'duration': len(audio_np) / self.dac_config.sampling_rate
            }
            
        except Exception as e:
            print(f"   âŒ DAC decoding failed: {e}")
            return self.fallback_mock_audio()
