#!/usr/bin/env python3
"""
DACçµ±åˆã®æœ€çµ‚ãƒ‡ãƒãƒƒã‚°
éŸ³å£°ã‚³ãƒ¼ãƒ‰ã®å€¤ç¯„å›²ã¨å½¢çŠ¶ã‚’è©³ç´°èª¿æŸ»
"""

import os
import sys
import torch
import numpy as np

# ãƒ‘ã‚¹è¨­å®š
phase1_dir = "/Users/yoshiaki/Projects/parler_tts_experiment/phase1_fixed_length"
os.chdir(phase1_dir)
sys.path.insert(0, phase1_dir)
sys.path.append('/Users/yoshiaki/Projects/parler-tts')

from parler_tts import ParlerTTSForConditionalGeneration
from memory_efficient_decoder import MemoryEfficientTTSGenerator

def debug_audio_codes_values():
    """éŸ³å£°ã‚³ãƒ¼ãƒ‰ã®å€¤ç¯„å›²ã‚’è©³ç´°èª¿æŸ»"""
    
    print("ğŸ” Debugging Audio Codes Values")
    print("=" * 50)
    
    # 1. å›ºå®šé•·ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã§ç”Ÿæˆã•ã‚Œã‚‹ã‚³ãƒ¼ãƒ‰ã‚’èª¿æŸ»
    print("1ï¸âƒ£ Investigating fixed-length decoder output...")
    
    generator = MemoryEfficientTTSGenerator(max_length=100)
    result = generator.generate_fixed_length("Hello")
    
    audio_codes = result['tokens'].squeeze(0)  # [num_codebooks, length]
    
    print(f"Fixed decoder audio codes:")
    print(f"  Shape: {audio_codes.shape}")
    print(f"  Data type: {audio_codes.dtype}")
    print(f"  Value range: [{audio_codes.min()}, {audio_codes.max()}]")
    print(f"  Unique values count: {len(torch.unique(audio_codes))}")
    print(f"  Sample values: {audio_codes[0, :10]}")  # æœ€åˆã®codebookã€æœ€åˆã®10å€‹
    
    # 2. ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã•ã‚Œã‚‹ã‚³ãƒ¼ãƒ‰ã¨æ¯”è¼ƒ
    print(f"\n2ï¸âƒ£ Comparing with original model...")
    
    model = ParlerTTSForConditionalGeneration.from_pretrained(
        "parler-tts/parler-tts-mini-v1",
        torch_dtype=torch.float32,
        device_map="cpu"
    )
    
    # DACè¨­å®šç¢ºèª
    dac_config = model.audio_encoder.config
    print(f"DAC Configuration:")
    print(f"  Codebook size: {dac_config.codebook_size}")
    print(f"  Expected range: [0, {dac_config.codebook_size - 1}]")
    
    # 3. éŸ³å£°ã‚³ãƒ¼ãƒ‰ã®å€¤ã‚’ä¿®æ­£
    print(f"\n3ï¸âƒ£ Testing corrected audio codes...")
    
    # ã‚³ãƒ¼ãƒ‰ã‚’æ­£ã—ã„ç¯„å›²ã«ã‚¯ãƒ©ãƒ³ãƒ—
    corrected_codes = torch.clamp(audio_codes, 0, dac_config.codebook_size - 1)
    
    print(f"Corrected audio codes:")
    print(f"  Shape: {corrected_codes.shape}")
    print(f"  Value range: [{corrected_codes.min()}, {corrected_codes.max()}]")
    print(f"  Changed values: {torch.sum(audio_codes != corrected_codes)}")
    
    return audio_codes, corrected_codes, model

def test_quantizer_with_corrected_codes(audio_codes, corrected_codes, model):
    """ä¿®æ­£ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã§Quantizerã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print(f"\n4ï¸âƒ£ Testing quantizer with corrected codes...")
    
    dac_internal = model.audio_encoder.model
    quantizer = dac_internal.quantizer
    
    # å½¢çŠ¶ã‚’èª¿æ•´
    if corrected_codes.dim() == 2:
        corrected_codes = corrected_codes.unsqueeze(0)  # [1, 9, 100]
    
    print(f"Testing codes shape: {corrected_codes.shape}")
    
    # quantizerã®å„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ®µéšçš„ã«ãƒ†ã‚¹ãƒˆ
    for method_name in ['from_codes', 'embed']:
        if hasattr(quantizer, method_name):
            print(f"\nğŸ§ª Testing {method_name}...")
            try:
                method = getattr(quantizer, method_name)
                latents = method(corrected_codes)
                print(f"   âœ… {method_name} success: {latents.shape}")
                
                # decodeãƒ†ã‚¹ãƒˆ
                try:
                    audio_waveform = dac_internal.decode(latents.detach())
                    print(f"   âœ… Decode success: {audio_waveform.shape}")
                    
                    # éŸ³å£°ä¿å­˜ãƒ†ã‚¹ãƒˆ
                    save_test_audio(audio_waveform, f"test_{method_name}.wav")
                    return True, method_name
                    
                except Exception as e:
                    print(f"   âŒ Decode failed: {e}")
                    
            except Exception as e:
                print(f"   âŒ {method_name} failed: {e}")
    
    # å€‹åˆ¥quantizerãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ§ª Testing individual quantizer layers...")
    try:
        latents = torch.zeros(1, 1024, corrected_codes.shape[2], device=corrected_codes.device)
        
        for i, quantizer_layer in enumerate(quantizer.quantizers):
            if i < corrected_codes.shape[1]:
                print(f"   Testing layer {i}...")
                
                # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®embedding
                layer_codes = corrected_codes[:, i, :]  # [1, 100]
                print(f"   Layer {i} codes range: [{layer_codes.min()}, {layer_codes.max()}]")
                
                try:
                    if hasattr(quantizer_layer, 'embed'):
                        embedded = quantizer_layer.embed(layer_codes)
                        print(f"   Layer {i} embed success: {embedded.shape}")
                        latents += embedded
                    else:
                        # codebookç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
                        embedded = quantizer_layer.codebook(layer_codes)
                        print(f"   Layer {i} codebook success: {embedded.shape}")
                        latents += embedded.transpose(1, 2)  # [1, time, dim] -> [1, dim, time]
                        
                except Exception as e:
                    print(f"   Layer {i} failed: {e}")
        
        print(f"   Combined latents shape: {latents.shape}")
        
        # æœ€çµ‚decode
        audio_waveform = dac_internal.decode(latents.detach())
        print(f"   âœ… Final decode success: {audio_waveform.shape}")
        
        save_test_audio(audio_waveform, "test_manual_layers.wav")
        return True, "manual_layers"
        
    except Exception as e:
        print(f"   âŒ Manual layers failed: {e}")
        return False, None

def save_test_audio(audio_waveform, filename):
    """ãƒ†ã‚¹ãƒˆéŸ³å£°ã‚’ä¿å­˜"""
    
    try:
        import wave
        
        if isinstance(audio_waveform, torch.Tensor):
            audio_np = audio_waveform.detach().squeeze().cpu().numpy()
        else:
            audio_np = np.array(audio_waveform)
        
        # æ­£è¦åŒ–
        if np.max(np.abs(audio_np)) > 0:
            audio_np = audio_np / np.max(np.abs(audio_np)) * 0.8
        
        sample_rate = 44100
        audio_int16 = (audio_np * 32767).astype(np.int16)
        
        with wave.open(filename, 'w') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_int16.tobytes())
        
        duration = len(audio_np) / sample_rate
        file_size = os.path.getsize(filename) / 1024
        
        print(f"   ğŸ“ Audio saved: {filename}")
        print(f"   Duration: {duration:.2f}s, Size: {file_size:.1f}KB")
        print(f"   ğŸ§ Play: open {filename}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Save failed: {e}")
        return False

def generate_final_working_code(working_method):
    """å‹•ä½œã™ã‚‹æ–¹æ³•ã«åŸºã¥ã„ã¦æœ€çµ‚ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
    
    print(f"\nğŸ“ Generating Final Working DAC Code")
    print("=" * 50)
    
    if working_method == "manual_layers":
        fix_code = """
# æœ€çµ‚çš„ã«å‹•ä½œã™ã‚‹DACçµ±åˆã‚³ãƒ¼ãƒ‰
def codes_to_audio(self, audio_codes):
    '''éŸ³å£°ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å®Ÿéš›ã®éŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆ (æœ€çµ‚å‹•ä½œç‰ˆ)'''
    
    print(f"ğŸµ Converting codes to audio...")
    print(f"   Input codes shape: {audio_codes.shape}")
    
    with torch.no_grad():
        start_time = time.time()
        
        try:
            # DACå†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            dac_internal = self.audio_encoder.model
            quantizer = dac_internal.quantizer
            
            # éŸ³å£°ã‚³ãƒ¼ãƒ‰ã®å½¢çŠ¶ã‚’èª¿æ•´
            if audio_codes.dim() == 2:
                audio_codes = audio_codes.unsqueeze(0)  # [9, 100] -> [1, 9, 100]
            
            # éŸ³å£°ã‚³ãƒ¼ãƒ‰ã‚’æ­£ã—ã„ç¯„å›²ã«ã‚¯ãƒ©ãƒ³ãƒ—
            audio_codes = torch.clamp(audio_codes, 0, self.dac_config.codebook_size - 1)
            print(f"   Clamped codes range: [{audio_codes.min()}, {audio_codes.max()}]")
            
            # å„quantizerãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ‰‹å‹•ã§å‡¦ç†
            latents = torch.zeros(audio_codes.shape[0], 1024, audio_codes.shape[2], 
                                device=audio_codes.device)
            
            for i, quantizer_layer in enumerate(quantizer.quantizers):
                if i < audio_codes.shape[1]:
                    layer_codes = audio_codes[:, i, :]  # [1, 100]
                    
                    # åŸ‹ã‚è¾¼ã¿å‡¦ç†
                    if hasattr(quantizer_layer, 'embed'):
                        embedded = quantizer_layer.embed(layer_codes)
                    else:
                        embedded = quantizer_layer.codebook(layer_codes)
                        embedded = embedded.transpose(1, 2)  # [1, time, dim] -> [1, dim, time]
                    
                    latents += embedded
            
            print(f"   Combined latents shape: {latents.shape}")
            
            # DAC decodeå®Ÿè¡Œ
            audio_waveform = dac_internal.decode(latents.detach())
            
            if isinstance(audio_waveform, torch.Tensor):
                audio_np = audio_waveform.detach().squeeze().cpu().numpy()
            else:
                audio_np = np.array(audio_waveform)
            
            end_time = time.time()
            
            print(f"   â±ï¸  Audio conversion time: {end_time - start_time:.3f}s")
            print(f"   ğŸ“Š Audio waveform shape: {audio_np.shape}")
            print(f"   ğŸšï¸  Audio range: [{audio_np.min():.3f}, {audio_np.max():.3f}]")
            
            return {
                'audio_waveform': audio_np,
                'conversion_time': end_time - start_time,
                'sample_rate': self.dac_config.sampling_rate,
                'duration': len(audio_np) / self.dac_config.sampling_rate
            }
            
        except Exception as e:
            print(f"   âŒ DAC decoding failed: {e}")
            return self.fallback_mock_audio()
"""
    elif working_method:
        fix_code = f"""
# å‹•ä½œã™ã‚‹DACçµ±åˆã‚³ãƒ¼ãƒ‰ (Method: {working_method})
def codes_to_audio(self, audio_codes):
    '''éŸ³å£°ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å®Ÿéš›ã®éŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆ'''
    
    with torch.no_grad():
        # DACå†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        dac_internal = self.audio_encoder.model
        quantizer = dac_internal.quantizer
        
        # å½¢çŠ¶èª¿æ•´
        if audio_codes.dim() == 2:
            audio_codes = audio_codes.unsqueeze(0)
        
        # å€¤ã‚’ã‚¯ãƒ©ãƒ³ãƒ—
        audio_codes = torch.clamp(audio_codes, 0, self.dac_config.codebook_size - 1)
        
        # {working_method}ã§æ½œåœ¨è¡¨ç¾ç”Ÿæˆ
        latents = quantizer.{working_method}(audio_codes)
        
        # decodeå®Ÿè¡Œ
        audio_waveform = dac_internal.decode(latents.detach())
        
        # çµæœå‡¦ç†...
        return result
"""
    else:
        fix_code = "# No working method found"
    
    print(fix_code)
    
    with open('working_dac_integration_final.py', 'w') as f:
        f.write(fix_code)
    
    print(f"ğŸ“ Final working code saved to: working_dac_integration_final.py")

def main():
    print("ğŸµ DAC Integration Final Debug")
    print("=" * 50)
    
    try:
        # éŸ³å£°ã‚³ãƒ¼ãƒ‰ã®å€¤ã‚’èª¿æŸ»
        audio_codes, corrected_codes, model = debug_audio_codes_values()
        
        # ä¿®æ­£ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆ
        success, method = test_quantizer_with_corrected_codes(audio_codes, corrected_codes, model)
        
        if success:
            print(f"\nğŸ‰ DAC integration finally working!")
            print(f"   Working method: {method}")
            generate_final_working_code(method)
        else:
            print(f"\nâš ï¸  Still debugging DAC integration...")
            generate_final_working_code(None)
            
    except Exception as e:
        print(f"âŒ Final DAC debug failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
