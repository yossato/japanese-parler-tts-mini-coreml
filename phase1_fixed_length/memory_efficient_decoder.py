#!/usr/bin/env python3
"""
Phase 1.1: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå›ºå®šé•·ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
å…ƒã®fixed_length_decoder.pyã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…å‰Šæ¸›ï¼ˆ20.8GB -> 4GBä»¥ä¸‹ï¼‰
"""

import sys
import os
sys.path.append('/Users/yoshiaki/Projects/parler-tts')

import torch
import torch.nn as nn
from transformers import AutoTokenizer
from parler_tts import ParlerTTSForConditionalGeneration
import time
from torch.profiler import profile, record_function, ProfilerActivity
import matplotlib.pyplot as plt
import json

class MemoryEfficientFixedLengthDecoder(nn.Module):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå›ºå®šé•·ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, original_decoder, max_length=100):  # 500->100ã«å‰Šæ¸›
        super().__init__()
        self.original_decoder = original_decoder
        self.max_length = max_length
        self.num_codebooks = original_decoder.config.num_codebooks
        self.vocab_size = original_decoder.config.vocab_size
        self.hidden_size = original_decoder.config.hidden_size
        
        print(f"ğŸ¯ Memory Efficient Decoder Config:")
        print(f"  Max length: {max_length} (vs original 500)")
        print(f"  Num codebooks: {self.num_codebooks}")
        print(f"  Vocab size: {self.vocab_size}")
        print(f"  Hidden size: {self.hidden_size}")
        
        # é•·ã•äºˆæ¸¬å™¨ï¼ˆå°ã•ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰
        self.length_predictor = nn.Sequential(
            nn.Linear(self.hidden_size, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
        # å„codebookç”¨ã®åŠ¹ç‡çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        # å…ƒã®å®Ÿè£…ï¼šhidden_size -> max_length * vocab_size (å·¨å¤§)
        # æ–°å®Ÿè£…ï¼šhidden_size -> ä¸­é–“å±¤ -> max_length * vocab_size (å°ã•ãªä¸­é–“å±¤)
        self.codebook_projectors = nn.ModuleList()
        
        for i in range(self.num_codebooks):
            projector = nn.Sequential(
                nn.Linear(self.hidden_size, 512),  # ä¸­é–“å±¤ã§åœ§ç¸®
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(512, 256),  # ã•ã‚‰ã«åœ§ç¸®
                nn.ReLU(),
                nn.Linear(256, max_length * self.vocab_size)  # æœ€çµ‚å‡ºåŠ›
            )
            self.codebook_projectors.append(projector)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¨ˆç®—ã¨è¡¨ç¤º
        total_params = sum(p.numel() for p in self.parameters())
        memory_mb = total_params * 4 / (1024**2)  # Float32
        
        print(f"ğŸ“Š Memory Efficient Decoder Stats:")
        print(f"  Parameters: {total_params:,}")
        print(f"  Estimated memory: {memory_mb:.1f} MB")
        
    def forward(self, encoder_hidden_states, encoder_attention_mask, 
                max_new_tokens=None, **kwargs):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªé †ä¼æ’­"""
        
        batch_size, encoder_seq_len, hidden_size = encoder_hidden_states.shape
        
        if max_new_tokens is None:
            max_new_tokens = self.max_length
        
        # å®Ÿéš›ã®é•·ã•ã‚’çŸ­ã‚ã«åˆ¶é™
        actual_length = min(max_new_tokens, self.max_length)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å‡ºåŠ›ã‚’pooling
        pooled_encoder = encoder_hidden_states.mean(dim=1)  # [batch_size, hidden_size]
        
        # é•·ã•äºˆæ¸¬
        predicted_length_ratio = self.length_predictor(pooled_encoder)
        predicted_length = (predicted_length_ratio * actual_length).round().int()
        
        # å„codebookã®å‡ºåŠ›ã‚’ç”Ÿæˆ
        outputs = []
        codebook_logits = []
        
        for codebook_idx in range(self.num_codebooks):
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            projected = self.codebook_projectors[codebook_idx](pooled_encoder)
            
            # [batch_size, actual_length, vocab_size]ã«reshape
            codebook_logits_reshaped = projected.view(batch_size, actual_length, self.vocab_size)
            codebook_logits.append(codebook_logits_reshaped)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³IDå–å¾—
            codebook_tokens = torch.argmax(codebook_logits_reshaped, dim=-1)
            outputs.append(codebook_tokens)
        
        # [batch_size, num_codebooks, actual_length]
        output_tokens = torch.stack(outputs, dim=1)
        
        return {
            'sequences': output_tokens.view(batch_size * self.num_codebooks, actual_length),
            'predicted_length': predicted_length,
            'logits': torch.stack(codebook_logits, dim=1),  # [batch_size, num_codebooks, length, vocab_size]
            'tokens': output_tokens,  # [batch_size, num_codebooks, length]
            'actual_length': actual_length
        }

class MemoryEfficientTTSGenerator:
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªTTSç”Ÿæˆå™¨"""
    
    def __init__(self, model_name="parler-tts/parler-tts-mini-v1", max_length=100):
        print(f"ğŸš€ Loading memory efficient model: {model_name}")
        print(f"   Max length: {max_length}")
        
        self.device = torch.device("cpu")  # ANEæ¤œè¨¼ã®ãŸã‚CPUã‹ã‚‰é–‹å§‹
        self.max_length = max_length
        
        # å…ƒã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        print("Loading original Parler TTS model...")
        self.model = ParlerTTSForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå›ºå®šé•·ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã«ç½®ãæ›ãˆ
        print("Creating memory efficient decoder...")
        self.efficient_decoder = MemoryEfficientFixedLengthDecoder(
            self.model.decoder,
            max_length=max_length
        )
        
        print("âœ… Memory efficient model loaded successfully!")
        
    def generate_original(self, text, description="A female speaker with a slightly low-pitched voice delivers her words quite expressively, in a very confined sounding environment with very clear audio quality."):
        """ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(
            description,
            text,
            return_tensors="pt",
            truncation=True,
            max_length=50
        )
        
        with torch.no_grad():
            # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®generateå®Ÿè¡Œï¼ˆParler TTSç”¨ã®æ­£ã—ã„å‘¼ã³å‡ºã—ï¼‰
            start_time = time.time()
            try:
                # Parler TTSã®æ­£ã—ã„ç”Ÿæˆæ–¹æ³•
                generation = self.model.generate(
                    input_ids=inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    prompt_input_ids=inputs.input_ids,  # prompt_hidden_statesã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
                    prompt_attention_mask=inputs.attention_mask,
                    do_sample=True,
                    temperature=1.0,
                    max_new_tokens=self.max_length
                )
            except Exception as e:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡å˜ãªç”Ÿæˆ
                print(f"      Full generation failed: {e}")
                print(f"      Trying simplified generation...")
                generation = self.model.generate(
                    input_ids=inputs.input_ids[:, :10],  # çŸ­ç¸®
                    do_sample=False,
                    max_new_tokens=min(50, self.max_length)
                )
            end_time = time.time()
            
        return {
            'sequences': generation,
            'generation_time': end_time - start_time,
            'method': 'original_autoregressive'
        }
        
    def generate_fixed_length(self, text, description="A female speaker with a slightly low-pitched voice delivers her words quite expressively, in a very confined sounding environment with very clear audio quality."):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå›ºå®šé•·ç”Ÿæˆ"""
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        text_inputs = self.tokenizer(
            description,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=50
        )
        
        prompt_inputs = self.tokenizer(
            text,
            return_tensors="pt", 
            padding=True,
            truncation=True,
            max_length=50
        )
        
        with torch.no_grad():
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ
            start_time = time.time()
            encoder_outputs = self.model.text_encoder(
                input_ids=text_inputs.input_ids,
                attention_mask=text_inputs.attention_mask
            )
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå›ºå®šé•·ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã§ç”Ÿæˆ
            decoder_outputs = self.efficient_decoder(
                encoder_hidden_states=encoder_outputs.last_hidden_state,
                encoder_attention_mask=text_inputs.attention_mask,
                max_new_tokens=self.max_length
            )
            end_time = time.time()
            
            decoder_outputs['generation_time'] = end_time - start_time
            decoder_outputs['method'] = 'memory_efficient_fixed_length'
            
        return decoder_outputs
    
    def compare_methods(self, texts, num_runs=3):
        """ã‚ªãƒªã‚¸ãƒŠãƒ« vs ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„å›ºå®šé•·ã®æ¯”è¼ƒ"""
        
        print(f"ğŸ”¬ Comparing Original vs Memory Efficient Fixed Length")
        print(f"   Texts: {len(texts)}")
        print(f"   Runs per text: {num_runs}")
        print(f"   Max length: {self.max_length}")
        
        results = {
            'texts': texts,
            'original_times': [],
            'fixed_times': [],
            'original_outputs': [],
            'fixed_outputs': [],
            'memory_usage': {},
            'max_length': self.max_length
        }
        
        for i, text in enumerate(texts):
            print(f"\nğŸ“ Text {i+1}: \"{text}\"")
            
            # ã‚ªãƒªã‚¸ãƒŠãƒ«ç”Ÿæˆã®æ¸¬å®š
            original_times = []
            try:
                print("  ğŸ”„ Testing original method...")
                for run in range(num_runs):
                    result = self.generate_original(text)
                    original_times.append(result['generation_time'])
                    if run == 0:
                        results['original_outputs'].append(result)
                        print(f"     Original shape: {result['sequences'].shape}")
            except Exception as e:
                print(f"     âš ï¸ Original method failed: {e}")
                original_times = [float('inf')] * num_runs
            
            # å›ºå®šé•·ç”Ÿæˆã®æ¸¬å®š
            fixed_times = []
            print("  âš¡ Testing memory efficient method...")
            for run in range(num_runs):
                result = self.generate_fixed_length(text)
                fixed_times.append(result['generation_time'])
                if run == 0:
                    results['fixed_outputs'].append(result)
                    print(f"     Fixed shape: {result['sequences'].shape}")
            
            # å¹³å‡æ™‚é–“ã®è¨ˆç®—
            avg_original = sum(original_times) / len(original_times) if original_times else float('inf')
            avg_fixed = sum(fixed_times) / len(fixed_times)
            
            results['original_times'].append(avg_original)
            results['fixed_times'].append(avg_fixed)
            
            # ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—è¨ˆç®—
            if avg_original != float('inf') and avg_fixed > 0:
                speedup = avg_original / avg_fixed
                print(f"     Original avg: {avg_original:.3f}s")
                print(f"     Fixed avg: {avg_fixed:.3f}s")
                print(f"     Speedup: {speedup:.1f}x")
            else:
                print(f"     Fixed avg: {avg_fixed:.3f}s")
                print(f"     Original failed or too slow")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®š
        original_params = sum(p.numel() for p in self.model.decoder.parameters())
        fixed_params = sum(p.numel() for p in self.efficient_decoder.parameters())
        
        results['memory_usage'] = {
            'original_params': original_params,
            'fixed_params': fixed_params,
            'original_memory_mb': original_params * 4 / (1024**2),
            'fixed_memory_mb': fixed_params * 4 / (1024**2),
            'memory_reduction_ratio': original_params / fixed_params if fixed_params > 0 else 0
        }
        
        print(f"\nğŸ“Š Memory Usage Comparison:")
        print(f"   Original: {results['memory_usage']['original_memory_mb']:.1f} MB")
        print(f"   Fixed: {results['memory_usage']['fixed_memory_mb']:.1f} MB")
        print(f"   Reduction: {results['memory_usage']['memory_reduction_ratio']:.1f}x")
        
        return results

def profile_memory_usage():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    
    print("\nğŸ” Memory Usage Profiling")
    print("=" * 40)
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š
    import psutil
    import gc
    
    process = psutil.Process()
    
    def get_memory_usage():
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        return process.memory_info().rss / (1024**2)  # MB
    
    # é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒª
    gc.collect()
    initial_memory = get_memory_usage()
    print(f"Initial memory: {initial_memory:.1f} MB")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å‰
    print("\nLoading model...")
    generator = MemoryEfficientTTSGenerator(max_length=100)
    
    after_load_memory = get_memory_usage()
    print(f"After model load: {after_load_memory:.1f} MB")
    print(f"Model loading overhead: {after_load_memory - initial_memory:.1f} MB")
    
    # ç”Ÿæˆå®Ÿè¡Œå‰
    print("\nGenerating sample...")
    result = generator.generate_fixed_length("Hello world")
    
    after_generation_memory = get_memory_usage()
    print(f"After generation: {after_generation_memory:.1f} MB")
    print(f"Generation overhead: {after_generation_memory - after_load_memory:.1f} MB")
    
    return {
        'initial_memory': initial_memory,
        'after_load_memory': after_load_memory,
        'after_generation_memory': after_generation_memory,
        'model_overhead': after_load_memory - initial_memory,
        'generation_overhead': after_generation_memory - after_load_memory
    }

def main():
    print("Phase 1.1: Memory Efficient Fixed Length TTS Decoder")
    print("=" * 60)
    
    try:
        # 1. ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
        memory_profile = profile_memory_usage()
        
        # 2. æ¯”è¼ƒå®Ÿé¨“
        print(f"\n{'='*60}")
        generator = MemoryEfficientTTSGenerator(max_length=100)
        
        test_texts = [
            "Hello",
            "Hello world", 
            "This is a test"
        ]
        
        # 3. æ¯”è¼ƒå®Ÿè¡Œ
        comparison_results = generator.compare_methods(test_texts, num_runs=3)
        
        # 4. çµæœä¿å­˜ï¼ˆJSON serializableå½¢å¼ã«å¤‰æ›ï¼‰
        def make_json_serializable(obj):
            """Tensorã‚„NumPyã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’JSONå½¢å¼ã«å¤‰æ›"""
            if hasattr(obj, 'tolist'):  # Tensor or numpy array
                return obj.tolist()
            elif hasattr(obj, 'item'):  # scalar tensor
                return obj.item()
            elif isinstance(obj, dict):
                return {k: make_json_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [make_json_serializable(v) for v in obj]
            elif hasattr(obj, '__dict__'):  # Other objects with attributes
                return str(obj)
            else:
                return obj
        
        results = {
            'comparison': make_json_serializable(comparison_results),
            'memory_profile': make_json_serializable(memory_profile),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'config': {
                'max_length': 100,
                'model_name': 'parler-tts/parler-tts-mini-v1'
            }
        }
        
        with open('phase1_1_memory_efficient_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ Results saved to phase1_1_memory_efficient_results.json")
        
        # 5. ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"\nğŸ“‹ Summary:")
        avg_fixed_time = sum(comparison_results['fixed_times']) / len(comparison_results['fixed_times'])
        print(f"  Average fixed generation time: {avg_fixed_time:.3f}s")
        print(f"  Memory reduction: {comparison_results['memory_usage']['memory_reduction_ratio']:.1f}x")
        print(f"  Max output length: {comparison_results['max_length']}")
        
        print(f"\nâœ… Phase 1.1 (Memory Efficient) completed successfully!")
        print(f"   Ready for Step 2: DAC Integration")
        
        return results
        
    except Exception as e:
        print(f"âŒ Phase 1.1 experiment failed: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()
